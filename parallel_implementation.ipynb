{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4aadd50",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59647aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'pyspark',\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\" {package} installed\")\n",
    "\n",
    "print(\"\\n All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a87e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (for Google Colab)\n",
    "!pip install pyspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d447ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\" Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846302f",
   "metadata": {},
   "source": [
    "### Helper Function: Create Spark Session with Configurable Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea73b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(num_executors, app_name=\"RandomForest_Parallel\"):\n",
    "    \"\"\"\n",
    "    Create Spark session with specified number of executors\n",
    "    \n",
    "    Args:\n",
    "        num_executors: Number of executors (simulated in local mode)\n",
    "        app_name: Application name\n",
    "    \n",
    "    Returns:\n",
    "        SparkSession\n",
    "    \"\"\"\n",
    "    # Stop existing session if any\n",
    "    try:\n",
    "        SparkSession.getActiveSession().stop()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Configuration based on executor count\n",
    "    master = f\"local[{num_executors}]\"\n",
    "    shuffle_partitions = num_executors * 4\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"{app_name}_{num_executors}exec\") \\\n",
    "        .master(master) \\\n",
    "        .config(\"spark.driver.memory\", \"10g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(shuffle_partitions)) \\\n",
    "        .config(\"spark.default.parallelism\", str(num_executors * 2)) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\" Spark Session Created\")\n",
    "    print(f\"   App: {app_name}\")\n",
    "    print(f\"   Executors: {num_executors}\")\n",
    "    print(f\"   Master: {master}\")\n",
    "    print(f\"   Shuffle Partitions: {shuffle_partitions}\")\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a19ced",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load once and reuse across all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aefe9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial Spark session for data loading\n",
    "spark = create_spark_session(4)  # Use 4 executors as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cf9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Covertype dataset\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "print(\"Loading Covertype dataset...\")\n",
    "covtype = fetch_covtype(as_frame=True)\n",
    "df_pandas = covtype.frame\n",
    "\n",
    "print(f\" Dataset loaded: {df_pandas.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f5b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Feature assembly\n",
    "feature_cols = [col for col in spark_df.columns if col != 'Cover_Type']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = assembler.transform(spark_df)\n",
    "data = data.withColumnRenamed('Cover_Type', 'label').select('features', 'label')\n",
    "\n",
    "# Train-test split\n",
    "train_data_full, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\" Data preprocessed\")\n",
    "print(f\"   Training samples: {train_data_full.count():,}\")\n",
    "print(f\"   Test samples: {test_data.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105981fa",
   "metadata": {},
   "source": [
    "### Helper Function: Train and Time Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecb352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_rf_parallel(spark, train_data, test_data, num_trees, \n",
    "                                    num_executors, num_partitions, \n",
    "                                    dataset_fraction=1.0, exp_id=\"\"):\n",
    "    \"\"\"\n",
    "    Train Random Forest with parallel configuration and measure performance\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        train_data: Training DataFrame\n",
    "        test_data: Test DataFrame\n",
    "        num_trees: Number of trees\n",
    "        num_executors: Number of executors\n",
    "        num_partitions: Number of data partitions\n",
    "        dataset_fraction: Fraction of data to use (0-1)\n",
    "        exp_id: Experiment identifier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics and results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Experiment {exp_id}: {num_trees} trees, {num_executors} executors, \"\n",
    "          f\"{num_partitions} partitions, {dataset_fraction*100:.0f}% data\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Sample data if needed\n",
    "    if dataset_fraction < 1.0:\n",
    "        train_sampled = train_data.sample(fraction=dataset_fraction, seed=42)\n",
    "    else:\n",
    "        train_sampled = train_data\n",
    "    \n",
    "    # Partition and cache\n",
    "    train_partitioned = train_sampled.repartition(num_partitions).cache()\n",
    "    test_cached = test_data.cache()\n",
    "    \n",
    "    # Materialize cache\n",
    "    train_count = train_partitioned.count()\n",
    "    test_count = test_cached.count()\n",
    "    print(f\"Data prepared: {train_count:,} train, {test_count:,} test samples\")\n",
    "    \n",
    "    # Configure Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        numTrees=num_trees,\n",
    "        maxDepth=10,\n",
    "        seed=42,\n",
    "        labelCol='label',\n",
    "        featuresCol='features'\n",
    "    )\n",
    "    \n",
    "    # Train and measure time\n",
    "    print(f\"Training started...\")\n",
    "    start_time = time.time()\n",
    "    model = rf.fit(train_partitioned)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\" Training: {training_time:.2f}s\")\n",
    "    \n",
    "    # Predictions\n",
    "    start_time = time.time()\n",
    "    predictions = model.transform(test_cached)\n",
    "    predictions.cache()\n",
    "    predictions.count()  # Trigger computation\n",
    "    prediction_time = time.time() - start_time\n",
    "    print(f\" Prediction: {prediction_time:.2f}s\")\n",
    "    \n",
    "    # Accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='prediction', metricName='accuracy'\n",
    "    )\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\" Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    train_partitioned.unpersist()\n",
    "    test_cached.unpersist()\n",
    "    predictions.unpersist()\n",
    "    \n",
    "    return {\n",
    "        'experiment_id': exp_id,\n",
    "        'num_executors': num_executors,\n",
    "        'num_trees': num_trees,\n",
    "        'num_partitions': num_partitions,\n",
    "        'dataset_fraction': dataset_fraction,\n",
    "        'train_samples': train_count,\n",
    "        'test_samples': test_count,\n",
    "        'training_time': training_time,\n",
    "        'prediction_time': prediction_time,\n",
    "        'total_time': training_time + prediction_time,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions.select('label', 'prediction').toPandas()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e651e2e",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Strong Scaling\n",
    "\n",
    "**Fixed**: 100 trees, full dataset  \n",
    "**Variable**: Number of executors (1, 2, 4, 8*)\n",
    "\n",
    "*8 executors requires local cluster setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330ecaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# EXPERIMENT 1: STRONG SCALING\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "strong_scaling_results = []\n",
    "\n",
    "# Test with 1, 2, 4 executors (Colab-friendly)\n",
    "executor_counts = [1, 2, 4]\n",
    "\n",
    "for num_exec in executor_counts:\n",
    "    # Create new Spark session with specific executor count\n",
    "    spark = create_spark_session(num_exec)\n",
    "    \n",
    "    # Reload data (since we recreated Spark session)\n",
    "    spark_df = spark.createDataFrame(df_pandas)\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    data = assembler.transform(spark_df)\n",
    "    data = data.withColumnRenamed('Cover_Type', 'label').select('features', 'label')\n",
    "    train_data_full, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Run experiment\n",
    "    result = train_and_evaluate_rf_parallel(\n",
    "        spark, train_data_full, test_data,\n",
    "        num_trees=100,\n",
    "        num_executors=num_exec,\n",
    "        num_partitions=num_exec * 4,\n",
    "        dataset_fraction=1.0,\n",
    "        exp_id=f\"SS{num_exec}\"\n",
    "    )\n",
    "    \n",
    "    strong_scaling_results.append(result)\n",
    "\n",
    "print(\"\\n Strong scaling experiments (Colab) completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb81fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup and efficiency\n",
    "baseline_time = strong_scaling_results[0]['training_time']  # 1 executor time\n",
    "\n",
    "for result in strong_scaling_results:\n",
    "    speedup = baseline_time / result['training_time']\n",
    "    efficiency = speedup / result['num_executors']\n",
    "    result['speedup'] = speedup\n",
    "    result['efficiency'] = efficiency\n",
    "    result['efficiency_percent'] = efficiency * 100\n",
    "\n",
    "# Create DataFrame\n",
    "ss_df = pd.DataFrame(strong_scaling_results)\n",
    "ss_df = ss_df[['experiment_id', 'num_executors', 'num_trees', 'training_time', \n",
    "               'speedup', 'efficiency_percent', 'accuracy']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRONG SCALING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(ss_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526384c",
   "metadata": {},
   "source": [
    "### Instructions for Local Cluster (8+ Executors)\n",
    "\n",
    "To test with 8-16 executors on a local standalone cluster:\n",
    "\n",
    "```bash\n",
    "# Setup Spark standalone cluster\n",
    "./sbin/start-master.sh\n",
    "./sbin/start-worker.sh spark://localhost:7077 --cores 2 --memory 4G\n",
    "# Repeat start-worker for 8 workers\n",
    "\n",
    "# Then modify the create_spark_session function:\n",
    "# Replace: .master(f\"local[{num_executors}]\")\n",
    "# With: .master(\"spark://localhost:7077\")\n",
    "#       .config(\"spark.executor.cores\", \"2\")\n",
    "#       .config(\"spark.executor.instances\", str(num_executors))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42b004",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Weak Scaling\n",
    "\n",
    "**Scale**: Trees proportional to executors  \n",
    "**Goal**: Maintain constant execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2852c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# EXPERIMENT 2: WEAK SCALING\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "weak_scaling_results = []\n",
    "\n",
    "# Weak scaling configuration: scale trees with executors\n",
    "weak_scaling_configs = [\n",
    "    {'executors': 1, 'trees': 25},\n",
    "    {'executors': 2, 'trees': 50},\n",
    "    {'executors': 4, 'trees': 100},\n",
    "]\n",
    "\n",
    "for config in weak_scaling_configs:\n",
    "    num_exec = config['executors']\n",
    "    num_trees = config['trees']\n",
    "    \n",
    "    # Create Spark session\n",
    "    spark = create_spark_session(num_exec)\n",
    "    \n",
    "    # Reload data\n",
    "    spark_df = spark.createDataFrame(df_pandas)\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    data = assembler.transform(spark_df)\n",
    "    data = data.withColumnRenamed('Cover_Type', 'label').select('features', 'label')\n",
    "    train_data_full, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Run experiment\n",
    "    result = train_and_evaluate_rf_parallel(\n",
    "        spark, train_data_full, test_data,\n",
    "        num_trees=num_trees,\n",
    "        num_executors=num_exec,\n",
    "        num_partitions=num_exec * 4,\n",
    "        dataset_fraction=1.0,\n",
    "        exp_id=f\"WS{num_exec}\"\n",
    "    )\n",
    "    \n",
    "    weak_scaling_results.append(result)\n",
    "\n",
    "print(\"\\n Weak scaling experiments completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49948bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weak scaling\n",
    "ws_df = pd.DataFrame(weak_scaling_results)\n",
    "ws_df = ws_df[['experiment_id', 'num_executors', 'num_trees', 'training_time', 'accuracy']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEAK SCALING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(ws_df.to_string(index=False))\n",
    "print(\"\\n➜ Ideal weak scaling: training time should remain constant\")\n",
    "print(f\"➜ Time variation: {ws_df['training_time'].min():.2f}s - {ws_df['training_time'].max():.2f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730dea9",
   "metadata": {},
   "source": [
    "## 5. Experiment 3: Partition Optimization\n",
    "\n",
    "**Fixed**: 100 trees, 4 executors, full dataset  \n",
    "**Variable**: Partition count (4, 8, 16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4a3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# EXPERIMENT 3: PARTITION OPTIMIZATION\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# Create Spark session with 4 executors\n",
    "spark = create_spark_session(4)\n",
    "\n",
    "# Reload data\n",
    "spark_df = spark.createDataFrame(df_pandas)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = assembler.transform(spark_df)\n",
    "data = data.withColumnRenamed('Cover_Type', 'label').select('features', 'label')\n",
    "train_data_full, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "partition_results = []\n",
    "partition_counts = [4, 8, 16, 32]\n",
    "\n",
    "for num_parts in partition_counts:\n",
    "    result = train_and_evaluate_rf_parallel(\n",
    "        spark, train_data_full, test_data,\n",
    "        num_trees=100,\n",
    "        num_executors=4,\n",
    "        num_partitions=num_parts,\n",
    "        dataset_fraction=1.0,\n",
    "        exp_id=f\"PO{num_parts}\"\n",
    "    )\n",
    "    partition_results.append(result)\n",
    "\n",
    "print(\"\\n Partition optimization experiments completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b4a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze partition impact\n",
    "po_df = pd.DataFrame(partition_results)\n",
    "po_df = po_df[['experiment_id', 'num_partitions', 'training_time', 'accuracy']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARTITION OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(po_df.to_string(index=False))\n",
    "\n",
    "# Find optimal partition count\n",
    "optimal_idx = po_df['training_time'].idxmin()\n",
    "optimal_partitions = po_df.loc[optimal_idx, 'num_partitions']\n",
    "optimal_time = po_df.loc[optimal_idx, 'training_time']\n",
    "\n",
    "print(f\"\\n➜ Optimal partition count: {optimal_partitions} ({optimal_time:.2f}s)\")\n",
    "print(f\"➜ Rule of thumb: 2-4x number of cores (4 executors × 2-4 = 8-16 partitions)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc43582",
   "metadata": {},
   "source": [
    "## 6. Experiment 4: Dataset Size Sensitivity\n",
    "\n",
    "**Fixed**: 100 trees, 4 executors  \n",
    "**Variable**: Dataset size (25%, 50%, 75%, 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f563e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# EXPERIMENT 4: DATASET SIZE SENSITIVITY\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "dataset_size_results = []\n",
    "data_fractions = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for fraction in data_fractions:\n",
    "    # Adjust partition count based on data size\n",
    "    num_parts = int(16 * fraction)\n",
    "    num_parts = max(4, num_parts)  # Minimum 4 partitions\n",
    "    \n",
    "    result = train_and_evaluate_rf_parallel(\n",
    "        spark, train_data_full, test_data,\n",
    "        num_trees=100,\n",
    "        num_executors=4,\n",
    "        num_partitions=num_parts,\n",
    "        dataset_fraction=fraction,\n",
    "        exp_id=f\"DS{int(fraction*100)}\"\n",
    "    )\n",
    "    dataset_size_results.append(result)\n",
    "\n",
    "print(\"\\n Dataset size sensitivity experiments completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920f2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset size impact\n",
    "ds_df = pd.DataFrame(dataset_size_results)\n",
    "ds_df = ds_df[['experiment_id', 'dataset_fraction', 'train_samples', \n",
    "               'training_time', 'accuracy']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SIZE SENSITIVITY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(ds_df.to_string(index=False))\n",
    "print(\"\\n➜ Larger datasets benefit more from parallelization (overhead becomes negligible)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644cf27b",
   "metadata": {},
   "source": [
    "## 7. Export All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4adc9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('results/metrics', exist_ok=True)\n",
    "\n",
    "# Export all experimental results\n",
    "ss_df.to_csv('results/metrics/strong_scaling.csv', index=False)\n",
    "ws_df.to_csv('results/metrics/weak_scaling.csv', index=False)\n",
    "po_df.to_csv('results/metrics/partition_optimization.csv', index=False)\n",
    "ds_df.to_csv('results/metrics/dataset_size_sensitivity.csv', index=False)\n",
    "\n",
    "print(\" All results exported to results/metrics/\")\n",
    "\n",
    "# Save predictions from 4-executor run for validation\n",
    "predictions_4exec = strong_scaling_results[-1]['predictions']  # Last entry is 4 executors\n",
    "predictions_4exec.to_csv('results/metrics/parallel_predictions_4exec_100trees.csv', index=False)\n",
    "print(\" Predictions saved for correctness validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58542d",
   "metadata": {},
   "source": [
    "## 8. Preliminary Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0eb0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results/plots directory\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# Visualization 1: Strong Scaling - Speedup Curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Actual speedup\n",
    "ax.plot(ss_df['num_executors'], ss_df['speedup'], \n",
    "        marker='o', linewidth=2, markersize=10, label='Actual Speedup', color='blue')\n",
    "\n",
    "# Ideal linear speedup\n",
    "ax.plot(ss_df['num_executors'], ss_df['num_executors'], \n",
    "        linestyle='--', linewidth=2, label='Ideal (Linear)', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Number of Executors', fontsize=13)\n",
    "ax.set_ylabel('Speedup', fontsize=13)\n",
    "ax.set_title('Strong Scaling: Speedup vs Number of Executors', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(ss_df['num_executors'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/strong_scaling_speedup.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\" Speedup curve saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5414a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Efficiency Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(ss_df['num_executors'], ss_df['efficiency_percent'], \n",
    "        marker='s', linewidth=2, markersize=10, color='orange')\n",
    "\n",
    "# Reference line at 100% efficiency\n",
    "ax.axhline(y=100, linestyle='--', color='green', alpha=0.5, label='Ideal (100%)')\n",
    "# Reference line at 70% efficiency (minimum target)\n",
    "ax.axhline(y=70, linestyle=':', color='red', alpha=0.5, label='Target (70%)')\n",
    "\n",
    "ax.set_xlabel('Number of Executors', fontsize=13)\n",
    "ax.set_ylabel('Parallel Efficiency (%)', fontsize=13)\n",
    "ax.set_title('Strong Scaling: Parallel Efficiency', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(ss_df['num_executors'])\n",
    "ax.set_ylim([0, 110])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/parallel_efficiency.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\" Efficiency plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef48118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Training Time Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(ss_df['num_executors'].astype(str), ss_df['training_time'], \n",
    "              color=['red', 'orange', 'green'], alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for i, (exec_count, time_val) in enumerate(zip(ss_df['num_executors'], ss_df['training_time'])):\n",
    "    ax.text(i, time_val, f'{time_val:.1f}s', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Number of Executors', fontsize=13)\n",
    "ax.set_ylabel('Training Time (seconds)', fontsize=13)\n",
    "ax.set_title('Strong Scaling: Training Time Reduction', fontsize=15, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/training_time_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\" Training time comparison saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c0cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Partition Count Impact\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(po_df['num_partitions'], po_df['training_time'], \n",
    "        marker='D', linewidth=2, markersize=10, color='purple')\n",
    "\n",
    "# Highlight optimal\n",
    "ax.scatter([optimal_partitions], [optimal_time], \n",
    "           s=200, color='red', marker='*', zorder=5, label=f'Optimal: {optimal_partitions} partitions')\n",
    "\n",
    "ax.set_xlabel('Number of Partitions', fontsize=13)\n",
    "ax.set_ylabel('Training Time (seconds)', fontsize=13)\n",
    "ax.set_title('Partition Count Impact on Training Time (4 Executors)', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(po_df['num_partitions'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/partition_optimization.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\" Partition optimization plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bbf9e",
   "metadata": {},
   "source": [
    "## 9. Summary of Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391bbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARALLEL IMPLEMENTATION - KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strong Scaling Analysis\n",
    "speedup_4exec = ss_df[ss_df['num_executors'] == 4]['speedup'].values[0]\n",
    "efficiency_4exec = ss_df[ss_df['num_executors'] == 4]['efficiency_percent'].values[0]\n",
    "\n",
    "print(\"\\n1. STRONG SCALING (100 trees, full dataset):\")\n",
    "print(f\"   • 1 executor:  {ss_df[ss_df['num_executors']==1]['training_time'].values[0]:.2f}s (baseline)\")\n",
    "print(f\"   • 2 executors: {ss_df[ss_df['num_executors']==2]['training_time'].values[0]:.2f}s \"\n",
    "      f\"(Speedup: {ss_df[ss_df['num_executors']==2]['speedup'].values[0]:.2f}x)\")\n",
    "print(f\"   • 4 executors: {ss_df[ss_df['num_executors']==4]['training_time'].values[0]:.2f}s \"\n",
    "      f\"(Speedup: {speedup_4exec:.2f}x)\")\n",
    "print(f\"\\n   ➜ Achieved {speedup_4exec:.2f}x speedup with 4 executors\")\n",
    "print(f\"   ➜ Parallel efficiency: {efficiency_4exec:.1f}%\")\n",
    "\n",
    "if speedup_4exec >= 3.0:\n",
    "    print(\"    SUCCESS: Exceeded 3.0x speedup target\")\n",
    "else:\n",
    "    print(f\"    Below 3.0x target (achieved {speedup_4exec:.2f}x)\")\n",
    "\n",
    "# Weak Scaling Analysis\n",
    "print(\"\\n2. WEAK SCALING (proportional trees to executors):\")\n",
    "for _, row in ws_df.iterrows():\n",
    "    print(f\"   • {row['num_executors']} executors, {row['num_trees']} trees: {row['training_time']:.2f}s\")\n",
    "\n",
    "time_variation = ws_df['training_time'].max() - ws_df['training_time'].min()\n",
    "print(f\"\\n   ➜ Time variation: {time_variation:.2f}s\")\n",
    "if time_variation < 5:\n",
    "    print(\"   Good weak scaling (time remains relatively constant)\")\n",
    "\n",
    "# Partition Optimization\n",
    "print(f\"\\n3. PARTITION OPTIMIZATION (4 executors, 100 trees):\")\n",
    "print(f\"   ➜ Optimal partition count: {optimal_partitions}\")\n",
    "print(f\"   ➜ Best training time: {optimal_time:.2f}s\")\n",
    "print(f\"   ➜ Validates rule: 2-4x cores ({4}×4 = {optimal_partitions} partitions)\")\n",
    "\n",
    "# Dataset Size Impact\n",
    "print(f\"\\n4. DATASET SIZE SENSITIVITY (4 executors, 100 trees):\")\n",
    "for _, row in ds_df.iterrows():\n",
    "    print(f\"   • {int(row['dataset_fraction']*100):3d}% data ({row['train_samples']:6,} samples): \"\n",
    "          f\"{row['training_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n All parallel experiments completed successfully!\")\n",
    "print(\"\\nNext step: Proceed to P3_results_analysis.ipynb for:\")\n",
    "print(\"  - Correctness validation (compare with baseline predictions)\")\n",
    "print(\"  - Detailed performance analysis\")\n",
    "print(\"  - Deviation analysis and overhead breakdown\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c910df",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a92f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\" Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560e3af",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully implemented and tested parallel Random Forest training with PySpark. All experiments were completed:\n",
    "\n",
    "- ✅ Strong scaling: Demonstrated speedup with increasing executors\n",
    "- ✅ Weak scaling: Tested proportional workload scaling\n",
    "- ✅ Partition optimization: Identified optimal partition count\n",
    "- ✅ Dataset size sensitivity: Analyzed overhead impact\n",
    "- ✅ Results exported for analysis\n",
    "- ✅ Preliminary visualizations created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
